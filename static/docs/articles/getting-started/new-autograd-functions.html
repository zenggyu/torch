<!DOCTYPE html>
<!-- Generated by pkgdown: do not edit by hand --><html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>Defining new autograd functions â€¢ torch</title>
<!-- jquery --><script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js" integrity="sha256-CSXorXvZcTkaix6Yvo6HppcZGetbYMGWSFlBw8HfCJo=" crossorigin="anonymous"></script><!-- Bootstrap --><link href="https://cdnjs.cloudflare.com/ajax/libs/bootswatch/3.4.0/united/bootstrap.min.css" rel="stylesheet" crossorigin="anonymous">
<script src="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/3.4.1/js/bootstrap.min.js" integrity="sha256-nuL8/2cJ5NDSSwnKD8VqreErSWHtnEP9E7AySL+1ev4=" crossorigin="anonymous"></script><!-- bootstrap-toc --><link rel="stylesheet" href="../../bootstrap-toc.css">
<script src="../../bootstrap-toc.js"></script><!-- Font Awesome icons --><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/all.min.css" integrity="sha256-mmgLkCYLUQbXn0B1SRqzHar6dCnv9oZFPEC1g1cwlkk=" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/v4-shims.min.css" integrity="sha256-wZjR52fzng1pJHwx4aV2AO3yyTOXrcDW7jBpJtTwVxw=" crossorigin="anonymous">
<!-- clipboard.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><!-- headroom.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.11.0/headroom.min.js" integrity="sha256-AsUX4SJE1+yuDu5+mAVzJbuYNPHj/WroHuZ8Ir/CkE0=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.11.0/jQuery.headroom.min.js" integrity="sha256-ZX/yNShbjqsohH1k95liqY9Gd8uOiE1S4vZc+9KQ1K4=" crossorigin="anonymous"></script><!-- pkgdown --><link href="../../pkgdown.css" rel="stylesheet">
<script src="../../pkgdown.js"></script><link href="../../extra.css" rel="stylesheet">
<meta property="og:title" content="Defining new autograd functions">
<meta property="og:description" content="torch">
<!-- mathjax --><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js" integrity="sha256-nvJJv9wWKEm88qvoQl9ekL2J+k/RWIsaSScxxlsrv8k=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/config/TeX-AMS-MML_HTMLorMML.js" integrity="sha256-84DKXVJXs0/F8OTMzX4UR909+jtl4G7SPypPavF+GfA=" crossorigin="anonymous"></script><!--[if lt IE 9]>
<script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js"></script>
<script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
<![endif]--><!-- Global site tag (gtag.js) - Google Analytics --><script async src="https://www.googletagmanager.com/gtag/js?id=UA-178883486-1"></script><script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-178883486-1');
</script>
</head>
<body data-spy="scroll" data-target="#toc">
    <div class="container template-article">
      <header><div class="navbar navbar-default navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <span class="navbar-brand">
        <a class="navbar-link" href="../../index.html">torch</a>
        <span class="version label label-default" data-toggle="tooltip" data-placement="bottom" title="Released version">0.1.1</span>
      </span>
    </div>

    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
<li>
  <a href="../../index.html">
    <span class="fas fa fas fa-home fa-lg"></span>
     
  </a>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    Getting started
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
<li class="dropdown-header">Beginers guide</li>
    <li>
      <a href="../../articles/getting-started/warmup.html">Warm-up</a>
    </li>
    <li>
      <a href="../../articles/getting-started/tensors.html">Tensors</a>
    </li>
    <li>
      <a href="../../articles/getting-started/tensors-and-autograd.html">Tensors and autograd</a>
    </li>
    <li>
      <a href="../../articles/getting-started/new-autograd-functions.html">Defining new autograd functions</a>
    </li>
    <li>
      <a href="../../articles/getting-started/nn.html">nn: neural networks with torch</a>
    </li>
    <li>
      <a href="../../articles/getting-started/optim.html">optim: optimizers in torch</a>
    </li>
    <li>
      <a href="../../articles/getting-started/custom-nn.html">Custom nn modules</a>
    </li>
    <li>
      <a href="../../articles/getting-started/control-flow-and-weight-sharing.html">Control flow &amp; Weight sharing</a>
    </li>
    <li class="dropdown-header">Torch Mechanics</li>
    <li>
      <a href="../../articles/getting-started/what-is-torch.html">What's torch?</a>
    </li>
    <li>
      <a href="../../articles/getting-started/autograd.html">Autograd: automatic differentiation</a>
    </li>
    <li>
      <a href="../../articles/getting-started/neural-networks.html">Neural networks</a>
    </li>
  </ul>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    Articles
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
<li class="dropdown-header">Tensors</li>
    <li>
      <a href="../../articles/tensor-creation.html">Creating tensors</a>
    </li>
    <li>
      <a href="../../articles/indexing.html">Indexing</a>
    </li>
    <li>
      <a href="../../articles/tensor/index.html">Tensor class</a>
    </li>
    <li>
      <a href="../../articles/serialization.html">Serialization</a>
    </li>
    <li class="dropdown-header">Datasets</li>
    <li>
      <a href="../../articles/loading-data.html">Loading Data</a>
    </li>
    <li class="dropdown-header">Autograd</li>
    <li>
      <a href="../../articles/using-autograd.html">Using autograd</a>
    </li>
    <li>
      <a href="../../articles/extending-autograd.html">Extending autograd</a>
    </li>
  </ul>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    Examples
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
<li>
      <a href="../../articles/examples/basic-autograd.html">basic-autograd</a>
    </li>
    <li>
      <a href="../../articles/examples/basic-nn-module.html">basic-nn-module</a>
    </li>
    <li>
      <a href="../../articles/examples/dataset.html">dataset</a>
    </li>
  </ul>
</li>
<li>
  <a href="../../reference/index.html">Reference</a>
</li>
<li>
  <a href="../../news/index.html">Changelog</a>
</li>
      </ul>
<ul class="nav navbar-nav navbar-right">
<li>
  <a href="https://github.com/mlverse/torch/">
    <span class="fab fa fab fa-github fa-lg"></span>
     
  </a>
</li>
      </ul>
</div>
<!--/.nav-collapse -->
  </div>
<!--/.container -->
</div>
<!--/.navbar -->

      

      </header><script src="new-autograd-functions_files/accessible-code-block-0.0.1/empty-anchor.js"></script><div class="row">
  <div class="col-md-9 contents">
    <div class="page-header toc-ignore">
      <h1 data-toc-skip>Defining new autograd functions</h1>
            
      
      <small class="dont-index">Source: <a href="https://github.com/mlverse/torch/blob/master/vignettes/getting-started/new-autograd-functions.Rmd"><code>vignettes/getting-started/new-autograd-functions.Rmd</code></a></small>
      <div class="hidden name"><code>new-autograd-functions.Rmd</code></div>

    </div>

    
    
<blockquote>
<p>Note: This is an R port of the official tutorial available <a href="https://pytorch.org/tutorials/beginner/examples_tensor/two_layer_net_numpy.html#sphx-glr-beginner-examples-tensor-two-layer-net-numpy-py">here</a>. All credits goes to <a href="https://github.com/jcjohnson/pytorch-examples">Justin Johnson</a>.</p>
</blockquote>
<div class="sourceCode" id="cb1"><pre class="downlit">
<span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://torch.mlverse.org/docs">torch</a></span><span class="op">)</span></pre></div>
<p>Under the hood, each primitive autograd operator is really two functions that operate on Tensors. The forward function computes output Tensors from input Tensors. The backward function receives the gradient of the output Tensors with respect to some scalar value, and computes the gradient of the input Tensors with respect to that same scalar value.</p>
<p>In torch we can easily define our own autograd operator by defining a subclass of <code>autograd_function</code> and implementing the forward and backward functions. We can then use our new autograd operator by constructing an instance and calling it like a function, passing Tensors containing input data.</p>
<p>In this example we define our own custom autograd function for performing the ReLU nonlinearity, and use it to implement our two-layer network:</p>
<div class="sourceCode" id="cb2"><pre class="downlit">
<span class="co"># We can implement our own custom autograd Functions by subclassing</span>
<span class="co"># autograd_functioon and implementing the forward and backward passes</span>
<span class="co"># which operate on Tensors.</span>
<span class="va">my_relu</span> <span class="op">&lt;-</span> <span class="fu"><a href="../../reference/autograd_function.html">autograd_function</a></span><span class="op">(</span>
   <span class="co"># In the forward pass we receive a Tensor containing the input and return</span>
   <span class="co"># a Tensor containing the output. ctx is a context object that can be used</span>
   <span class="co"># to stash information for backward computation. You can cache arbitrary</span>
   <span class="co"># objects for use in the backward pass using the ctx$save_for_backward method.</span>
   forward <span class="op">=</span> <span class="kw">function</span><span class="op">(</span><span class="va">ctx</span>, <span class="va">input</span><span class="op">)</span> <span class="op">{</span>
      <span class="va">ctx</span><span class="op">$</span><span class="fu">save_for_backward</span><span class="op">(</span>input <span class="op">=</span> <span class="va">input</span><span class="op">)</span>
      <span class="va">input</span><span class="op">$</span><span class="fu">clamp</span><span class="op">(</span>min <span class="op">=</span> <span class="fl">0</span><span class="op">)</span>
   <span class="op">}</span>,
   <span class="co"># In the backward pass we receive a Tensor containing the gradient of the loss</span>
   <span class="co"># with respect to the output, and we need to compute the gradient of the loss</span>
   <span class="co"># with respect to the input.</span>
   backward <span class="op">=</span> <span class="kw">function</span><span class="op">(</span><span class="va">ctx</span>, <span class="va">grad_output</span><span class="op">)</span> <span class="op">{</span>
      <span class="va">v</span> <span class="op">&lt;-</span> <span class="va">ctx</span><span class="op">$</span><span class="va">saved_variables</span>
      <span class="va">grad_input</span> <span class="op">&lt;-</span> <span class="va">grad_output</span><span class="op">$</span><span class="fu">clone</span><span class="op">(</span><span class="op">)</span>
      <span class="va">grad_input</span><span class="op">[</span><span class="va">v</span><span class="op">$</span><span class="va">input</span> <span class="op">&lt;</span> <span class="fl">0</span><span class="op">]</span> <span class="op">&lt;-</span> <span class="fl">0</span>
      <span class="fu"><a href="https://rdrr.io/r/base/list.html">list</a></span><span class="op">(</span>input <span class="op">=</span> <span class="va">grad_input</span><span class="op">)</span>
   <span class="op">}</span>
<span class="op">)</span>

<span class="kw">if</span> <span class="op">(</span><span class="fu"><a href="../../reference/cuda_is_available.html">cuda_is_available</a></span><span class="op">(</span><span class="op">)</span><span class="op">)</span> <span class="op">{</span>
   <span class="va">device</span> <span class="op">&lt;-</span> <span class="fu"><a href="../../reference/torch_device.html">torch_device</a></span><span class="op">(</span><span class="st">"cuda"</span><span class="op">)</span>
<span class="op">}</span> <span class="kw">else</span> <span class="op">{</span>
   <span class="va">device</span> <span class="op">&lt;-</span> <span class="fu"><a href="../../reference/torch_device.html">torch_device</a></span><span class="op">(</span><span class="st">"cpu"</span><span class="op">)</span>
<span class="op">}</span>
   
<span class="co"># N is batch size; D_in is input dimension;</span>
<span class="co"># H is hidden dimension; D_out is output dimension.</span>
<span class="va">N</span> <span class="op">&lt;-</span> <span class="fl">64</span>
<span class="va">D_in</span> <span class="op">&lt;-</span> <span class="fl">1000</span>
<span class="va">H</span> <span class="op">&lt;-</span> <span class="fl">100</span>
<span class="va">D_out</span> <span class="op">&lt;-</span> <span class="fl">10</span>

<span class="co"># Create random input and output data</span>
<span class="co"># Setting requires_grad=FALSE (the default) indicates that we do not need to </span>
<span class="co"># compute gradients with respect to these Tensors during the backward pass.</span>
<span class="va">x</span> <span class="op">&lt;-</span> <span class="fu"><a href="../../reference/torch_randn.html">torch_randn</a></span><span class="op">(</span><span class="va">N</span>, <span class="va">D_in</span>, device<span class="op">=</span><span class="va">device</span><span class="op">)</span>
<span class="va">y</span> <span class="op">&lt;-</span> <span class="fu"><a href="../../reference/torch_randn.html">torch_randn</a></span><span class="op">(</span><span class="va">N</span>, <span class="va">D_out</span>, device<span class="op">=</span><span class="va">device</span><span class="op">)</span>

<span class="co"># Randomly initialize weights</span>
<span class="co"># Setting requires_grad=TRUE indicates that we want to compute gradients with</span>
<span class="co"># respect to these Tensors during the backward pass.</span>
<span class="va">w1</span> <span class="op">&lt;-</span> <span class="fu"><a href="../../reference/torch_randn.html">torch_randn</a></span><span class="op">(</span><span class="va">D_in</span>, <span class="va">H</span>, device<span class="op">=</span><span class="va">device</span>, requires_grad <span class="op">=</span> <span class="cn">TRUE</span><span class="op">)</span>
<span class="va">w2</span> <span class="op">&lt;-</span> <span class="fu"><a href="../../reference/torch_randn.html">torch_randn</a></span><span class="op">(</span><span class="va">H</span>, <span class="va">D_out</span>, device<span class="op">=</span><span class="va">device</span>, requires_grad <span class="op">=</span> <span class="cn">TRUE</span><span class="op">)</span>

<span class="va">learning_rate</span> <span class="op">&lt;-</span> <span class="fl">1e-6</span>
<span class="kw">for</span> <span class="op">(</span><span class="va">t</span> <span class="kw">in</span> <span class="fu"><a href="https://rdrr.io/r/base/seq.html">seq_len</a></span><span class="op">(</span><span class="fl">500</span><span class="op">)</span><span class="op">)</span> <span class="op">{</span>
   <span class="co"># Forward pass: compute predicted y using operations on Tensors; these</span>
   <span class="co"># are exactly the same operations we used to compute the forward pass using</span>
   <span class="co"># Tensors, but we do not need to keep references to intermediate values since</span>
   <span class="co"># we are not implementing the backward pass by hand.</span>
   <span class="va">y_pred</span> <span class="op">&lt;-</span> <span class="fu">my_relu</span><span class="op">(</span><span class="va">x</span><span class="op">$</span><span class="fu">mm</span><span class="op">(</span><span class="va">w1</span><span class="op">)</span><span class="op">)</span><span class="op">$</span><span class="fu">mm</span><span class="op">(</span><span class="va">w2</span><span class="op">)</span>
   
   <span class="co"># Compute and print loss using operations on Tensors.</span>
   <span class="co"># Now loss is a Tensor of shape (1,)</span>
   <span class="va">loss</span> <span class="op">&lt;-</span> <span class="op">(</span><span class="va">y_pred</span> <span class="op">-</span> <span class="va">y</span><span class="op">)</span><span class="op">$</span><span class="fu">pow</span><span class="op">(</span><span class="fl">2</span><span class="op">)</span><span class="op">$</span><span class="fu">sum</span><span class="op">(</span><span class="op">)</span>
   <span class="kw">if</span> <span class="op">(</span><span class="va">t</span> <span class="op">%%</span> <span class="fl">100</span> <span class="op">==</span> <span class="fl">0</span> <span class="op">||</span> <span class="va">t</span> <span class="op">==</span> <span class="fl">1</span><span class="op">)</span>
      <span class="fu"><a href="https://rdrr.io/r/base/cat.html">cat</a></span><span class="op">(</span><span class="st">"Step:"</span>, <span class="va">t</span>, <span class="st">":"</span>, <span class="fu"><a href="https://rdrr.io/r/base/numeric.html">as.numeric</a></span><span class="op">(</span><span class="va">loss</span><span class="op">)</span>, <span class="st">"\n"</span><span class="op">)</span>
   
   <span class="co"># Use autograd to compute the backward pass. This call will compute the</span>
   <span class="co"># gradient of loss with respect to all Tensors with requires_grad=True.</span>
   <span class="co"># After this call w1$grad and w2$grad will be Tensors holding the gradient</span>
   <span class="co"># of the loss with respect to w1 and w2 respectively.</span>
   <span class="va">loss</span><span class="op">$</span><span class="fu">backward</span><span class="op">(</span><span class="op">)</span>
   
   <span class="co"># Manually update weights using gradient descent. Wrap in `with_no_grad`</span>
   <span class="co"># because weights have requires_grad=TRUE, but we don't need to track this</span>
   <span class="co"># in autograd.</span>
   <span class="co"># You can also use optim_sgd to achieve this.</span>
   <span class="fu"><a href="../../reference/with_no_grad.html">with_no_grad</a></span><span class="op">(</span><span class="op">{</span>
      
      <span class="co"># operations suffixed with an `_` operates on in-place on the tensor.</span>
      <span class="va">w1</span><span class="op">$</span><span class="fu">sub_</span><span class="op">(</span><span class="va">learning_rate</span> <span class="op">*</span> <span class="va">w1</span><span class="op">$</span><span class="va">grad</span><span class="op">)</span>
      <span class="va">w2</span><span class="op">$</span><span class="fu">sub_</span><span class="op">(</span><span class="va">learning_rate</span> <span class="op">*</span> <span class="va">w2</span><span class="op">$</span><span class="va">grad</span><span class="op">)</span>
      
      <span class="co"># Manually zero the gradients after updating weights</span>
      <span class="va">w1</span><span class="op">$</span><span class="va">grad</span><span class="op">$</span><span class="fu">zero_</span><span class="op">(</span><span class="op">)</span>
      <span class="va">w2</span><span class="op">$</span><span class="va">grad</span><span class="op">$</span><span class="fu">zero_</span><span class="op">(</span><span class="op">)</span>
   <span class="op">}</span><span class="op">)</span>
<span class="op">}</span>
<span class="co">#&gt; Step: 1 : 33210170 </span>
<span class="co">#&gt; Step: 100 : 429.2728 </span>
<span class="co">#&gt; Step: 200 : 2.536572 </span>
<span class="co">#&gt; Step: 300 : 0.02151564 </span>
<span class="co">#&gt; Step: 400 : 0.0003944117 </span>
<span class="co">#&gt; Step: 500 : 4.690059e-05</span></pre></div>
<p>In the <a href="nn.html">next example</a> we will learn how to use the neural networks abstractions in torch.</p>
  </div>

  <div class="col-md-3 hidden-xs hidden-sm" id="pkgdown-sidebar">

      </div>

</div>



      <footer><div class="copyright">
  <p>Developed by Daniel Falbel, Javier Luraschi.</p>
</div>

<div class="pkgdown">
  <p>Site built with <a href="https://pkgdown.r-lib.org/">pkgdown</a> 1.6.1.</p>
</div>

      </footer>
</div>

  


  </body>
</html>
