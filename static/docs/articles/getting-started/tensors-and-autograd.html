<!DOCTYPE html>
<!-- Generated by pkgdown: do not edit by hand --><html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>Tensors and autograd • torch</title>
<!-- jquery --><script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js" integrity="sha256-CSXorXvZcTkaix6Yvo6HppcZGetbYMGWSFlBw8HfCJo=" crossorigin="anonymous"></script><!-- Bootstrap --><link href="https://cdnjs.cloudflare.com/ajax/libs/bootswatch/3.4.0/united/bootstrap.min.css" rel="stylesheet" crossorigin="anonymous">
<script src="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/3.4.1/js/bootstrap.min.js" integrity="sha256-nuL8/2cJ5NDSSwnKD8VqreErSWHtnEP9E7AySL+1ev4=" crossorigin="anonymous"></script><!-- bootstrap-toc --><link rel="stylesheet" href="../../bootstrap-toc.css">
<script src="../../bootstrap-toc.js"></script><!-- Font Awesome icons --><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/all.min.css" integrity="sha256-mmgLkCYLUQbXn0B1SRqzHar6dCnv9oZFPEC1g1cwlkk=" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/v4-shims.min.css" integrity="sha256-wZjR52fzng1pJHwx4aV2AO3yyTOXrcDW7jBpJtTwVxw=" crossorigin="anonymous">
<!-- clipboard.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><!-- headroom.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.11.0/headroom.min.js" integrity="sha256-AsUX4SJE1+yuDu5+mAVzJbuYNPHj/WroHuZ8Ir/CkE0=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.11.0/jQuery.headroom.min.js" integrity="sha256-ZX/yNShbjqsohH1k95liqY9Gd8uOiE1S4vZc+9KQ1K4=" crossorigin="anonymous"></script><!-- pkgdown --><link href="../../pkgdown.css" rel="stylesheet">
<script src="../../pkgdown.js"></script><link href="../../extra.css" rel="stylesheet">
<meta property="og:title" content="Tensors and autograd">
<meta property="og:description" content="torch">
<!-- mathjax --><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js" integrity="sha256-nvJJv9wWKEm88qvoQl9ekL2J+k/RWIsaSScxxlsrv8k=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/config/TeX-AMS-MML_HTMLorMML.js" integrity="sha256-84DKXVJXs0/F8OTMzX4UR909+jtl4G7SPypPavF+GfA=" crossorigin="anonymous"></script><!--[if lt IE 9]>
<script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js"></script>
<script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
<![endif]--><!-- Global site tag (gtag.js) - Google Analytics --><script async src="https://www.googletagmanager.com/gtag/js?id=UA-178883486-1"></script><script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-178883486-1');
</script>
</head>
<body data-spy="scroll" data-target="#toc">
    <div class="container template-article">
      <header><div class="navbar navbar-default navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <span class="navbar-brand">
        <a class="navbar-link" href="../../index.html">torch</a>
        <span class="version label label-default" data-toggle="tooltip" data-placement="bottom" title="Released version">0.1.1</span>
      </span>
    </div>

    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
<li>
  <a href="../../index.html">
    <span class="fas fa fas fa-home fa-lg"></span>
     
  </a>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    Getting started
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
<li class="dropdown-header">Beginers guide</li>
    <li>
      <a href="../../articles/getting-started/warmup.html">Warm-up</a>
    </li>
    <li>
      <a href="../../articles/getting-started/tensors.html">Tensors</a>
    </li>
    <li>
      <a href="../../articles/getting-started/tensors-and-autograd.html">Tensors and autograd</a>
    </li>
    <li>
      <a href="../../articles/getting-started/new-autograd-functions.html">Defining new autograd functions</a>
    </li>
    <li>
      <a href="../../articles/getting-started/nn.html">nn: neural networks with torch</a>
    </li>
    <li>
      <a href="../../articles/getting-started/optim.html">optim: optimizers in torch</a>
    </li>
    <li>
      <a href="../../articles/getting-started/custom-nn.html">Custom nn modules</a>
    </li>
    <li>
      <a href="../../articles/getting-started/control-flow-and-weight-sharing.html">Control flow &amp; Weight sharing</a>
    </li>
    <li class="dropdown-header">Torch Mechanics</li>
    <li>
      <a href="../../articles/getting-started/what-is-torch.html">What's torch?</a>
    </li>
    <li>
      <a href="../../articles/getting-started/autograd.html">Autograd: automatic differentiation</a>
    </li>
    <li>
      <a href="../../articles/getting-started/neural-networks.html">Neural networks</a>
    </li>
  </ul>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    Articles
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
<li class="dropdown-header">Tensors</li>
    <li>
      <a href="../../articles/tensor-creation.html">Creating tensors</a>
    </li>
    <li>
      <a href="../../articles/indexing.html">Indexing</a>
    </li>
    <li>
      <a href="../../articles/tensor/index.html">Tensor class</a>
    </li>
    <li>
      <a href="../../articles/serialization.html">Serialization</a>
    </li>
    <li class="dropdown-header">Datasets</li>
    <li>
      <a href="../../articles/loading-data.html">Loading Data</a>
    </li>
    <li class="dropdown-header">Autograd</li>
    <li>
      <a href="../../articles/using-autograd.html">Using autograd</a>
    </li>
    <li>
      <a href="../../articles/extending-autograd.html">Extending autograd</a>
    </li>
  </ul>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    Examples
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
<li>
      <a href="../../articles/examples/basic-autograd.html">basic-autograd</a>
    </li>
    <li>
      <a href="../../articles/examples/basic-nn-module.html">basic-nn-module</a>
    </li>
    <li>
      <a href="../../articles/examples/dataset.html">dataset</a>
    </li>
  </ul>
</li>
<li>
  <a href="../../reference/index.html">Reference</a>
</li>
<li>
  <a href="../../news/index.html">Changelog</a>
</li>
      </ul>
<ul class="nav navbar-nav navbar-right">
<li>
  <a href="https://github.com/mlverse/torch/">
    <span class="fab fa fab fa-github fa-lg"></span>
     
  </a>
</li>
      </ul>
</div>
<!--/.nav-collapse -->
  </div>
<!--/.container -->
</div>
<!--/.navbar -->

      

      </header><script src="tensors-and-autograd_files/accessible-code-block-0.0.1/empty-anchor.js"></script><div class="row">
  <div class="col-md-9 contents">
    <div class="page-header toc-ignore">
      <h1 data-toc-skip>Tensors and autograd</h1>
            
      
      <small class="dont-index">Source: <a href="https://github.com/mlverse/torch/blob/master/vignettes/getting-started/tensors-and-autograd.Rmd"><code>vignettes/getting-started/tensors-and-autograd.Rmd</code></a></small>
      <div class="hidden name"><code>tensors-and-autograd.Rmd</code></div>

    </div>

    
    
<blockquote>
<p>Note: This is an R port of the official tutorial available <a href="https://pytorch.org/tutorials/beginner/examples_tensor/two_layer_net_numpy.html#sphx-glr-beginner-examples-tensor-two-layer-net-numpy-py">here</a>. All credits goes to <a href="https://github.com/jcjohnson/pytorch-examples">Justin Johnson</a>.</p>
</blockquote>
<div class="sourceCode" id="cb1"><pre class="downlit">
<span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://torch.mlverse.org/docs">torch</a></span><span class="op">)</span></pre></div>
<p>In the previous examples, we had to manually implement both the forward and backward passes of our neural network. Manually implementing the backward pass is not a big deal for a small two-layer network, but can quickly get very hairy for large complex networks.</p>
<p>Thankfully, we can use automatic differentiation to automate the computation of backward passes in neural networks. The autograd feature in torch provides exactly this functionality. When using autograd, the forward pass of your network will define a computational graph; nodes in the graph will be Tensors, and edges will be functions that produce output Tensors from input Tensors. Backpropagating through this graph then allows you to easily compute gradients.</p>
<p>This sounds complicated, it’s pretty simple to use in practice. Each Tensor represents a node in a computational graph. If x is a Tensor that has <code>x$requires_grad=TRUE</code> then <code>x$grad</code> is another Tensor holding the gradient of x with respect to some scalar value.</p>
<p>Here we use torch Tensors and autograd to implement our two-layer network; now we no longer need to manually implement the backward pass through the network:</p>
<div class="sourceCode" id="cb2"><pre class="downlit">
<span class="kw">if</span> <span class="op">(</span><span class="fu"><a href="../../reference/cuda_is_available.html">cuda_is_available</a></span><span class="op">(</span><span class="op">)</span><span class="op">)</span> <span class="op">{</span>
   <span class="va">device</span> <span class="op">&lt;-</span> <span class="fu"><a href="../../reference/torch_device.html">torch_device</a></span><span class="op">(</span><span class="st">"cuda"</span><span class="op">)</span>
<span class="op">}</span> <span class="kw">else</span> <span class="op">{</span>
   <span class="va">device</span> <span class="op">&lt;-</span> <span class="fu"><a href="../../reference/torch_device.html">torch_device</a></span><span class="op">(</span><span class="st">"cpu"</span><span class="op">)</span>
<span class="op">}</span>
   
<span class="co"># N is batch size; D_in is input dimension;</span>
<span class="co"># H is hidden dimension; D_out is output dimension.</span>
<span class="va">N</span> <span class="op">&lt;-</span> <span class="fl">64</span>
<span class="va">D_in</span> <span class="op">&lt;-</span> <span class="fl">1000</span>
<span class="va">H</span> <span class="op">&lt;-</span> <span class="fl">100</span>
<span class="va">D_out</span> <span class="op">&lt;-</span> <span class="fl">10</span>

<span class="co"># Create random input and output data</span>
<span class="co"># Setting requires_grad=FALSE (the default) indicates that we do not need to </span>
<span class="co"># compute gradients with respect to these Tensors during the backward pass.</span>
<span class="va">x</span> <span class="op">&lt;-</span> <span class="fu"><a href="../../reference/torch_randn.html">torch_randn</a></span><span class="op">(</span><span class="va">N</span>, <span class="va">D_in</span>, device<span class="op">=</span><span class="va">device</span><span class="op">)</span>
<span class="va">y</span> <span class="op">&lt;-</span> <span class="fu"><a href="../../reference/torch_randn.html">torch_randn</a></span><span class="op">(</span><span class="va">N</span>, <span class="va">D_out</span>, device<span class="op">=</span><span class="va">device</span><span class="op">)</span>

<span class="co"># Randomly initialize weights</span>
<span class="co"># Setting requires_grad=TRUE indicates that we want to compute gradients with</span>
<span class="co"># respect to these Tensors during the backward pass.</span>
<span class="va">w1</span> <span class="op">&lt;-</span> <span class="fu"><a href="../../reference/torch_randn.html">torch_randn</a></span><span class="op">(</span><span class="va">D_in</span>, <span class="va">H</span>, device<span class="op">=</span><span class="va">device</span>, requires_grad <span class="op">=</span> <span class="cn">TRUE</span><span class="op">)</span>
<span class="va">w2</span> <span class="op">&lt;-</span> <span class="fu"><a href="../../reference/torch_randn.html">torch_randn</a></span><span class="op">(</span><span class="va">H</span>, <span class="va">D_out</span>, device<span class="op">=</span><span class="va">device</span>, requires_grad <span class="op">=</span> <span class="cn">TRUE</span><span class="op">)</span>

<span class="va">learning_rate</span> <span class="op">&lt;-</span> <span class="fl">1e-6</span>
<span class="kw">for</span> <span class="op">(</span><span class="va">t</span> <span class="kw">in</span> <span class="fu"><a href="https://rdrr.io/r/base/seq.html">seq_len</a></span><span class="op">(</span><span class="fl">500</span><span class="op">)</span><span class="op">)</span> <span class="op">{</span>
   <span class="co"># Forward pass: compute predicted y using operations on Tensors; these</span>
   <span class="co"># are exactly the same operations we used to compute the forward pass using</span>
   <span class="co"># Tensors, but we do not need to keep references to intermediate values since</span>
   <span class="co"># we are not implementing the backward pass by hand.</span>
   <span class="va">y_pred</span> <span class="op">&lt;-</span> <span class="va">x</span><span class="op">$</span><span class="fu">mm</span><span class="op">(</span><span class="va">w1</span><span class="op">)</span><span class="op">$</span><span class="fu">clamp</span><span class="op">(</span>min<span class="op">=</span><span class="fl">0</span><span class="op">)</span><span class="op">$</span><span class="fu">mm</span><span class="op">(</span><span class="va">w2</span><span class="op">)</span>
   
   <span class="co"># Compute and print loss using operations on Tensors.</span>
   <span class="co"># Now loss is a Tensor of shape (1,)</span>
   <span class="va">loss</span> <span class="op">&lt;-</span> <span class="op">(</span><span class="va">y_pred</span> <span class="op">-</span> <span class="va">y</span><span class="op">)</span><span class="op">$</span><span class="fu">pow</span><span class="op">(</span><span class="fl">2</span><span class="op">)</span><span class="op">$</span><span class="fu">sum</span><span class="op">(</span><span class="op">)</span>
   <span class="kw">if</span> <span class="op">(</span><span class="va">t</span> <span class="op">%%</span> <span class="fl">100</span> <span class="op">==</span> <span class="fl">0</span> <span class="op">||</span> <span class="va">t</span> <span class="op">==</span> <span class="fl">1</span><span class="op">)</span>
      <span class="fu"><a href="https://rdrr.io/r/base/cat.html">cat</a></span><span class="op">(</span><span class="st">"Step:"</span>, <span class="va">t</span>, <span class="st">":"</span>, <span class="fu"><a href="https://rdrr.io/r/base/numeric.html">as.numeric</a></span><span class="op">(</span><span class="va">loss</span><span class="op">)</span>, <span class="st">"\n"</span><span class="op">)</span>
   
   <span class="co"># Use autograd to compute the backward pass. This call will compute the</span>
   <span class="co"># gradient of loss with respect to all Tensors with requires_grad=True.</span>
   <span class="co"># After this call w1$grad and w2$grad will be Tensors holding the gradient</span>
   <span class="co"># of the loss with respect to w1 and w2 respectively.</span>
   <span class="va">loss</span><span class="op">$</span><span class="fu">backward</span><span class="op">(</span><span class="op">)</span>
   
   <span class="co"># Manually update weights using gradient descent. Wrap in `with_no_grad`</span>
   <span class="co"># because weights have requires_grad=TRUE, but we don't need to track this</span>
   <span class="co"># in autograd.</span>
   <span class="co"># You can also use optim_sgd to achieve this.</span>
   <span class="fu"><a href="../../reference/with_no_grad.html">with_no_grad</a></span><span class="op">(</span><span class="op">{</span>
      
      <span class="co"># operations suffixed with an `_` operates on in-place on the tensor.</span>
      <span class="va">w1</span><span class="op">$</span><span class="fu">sub_</span><span class="op">(</span><span class="va">learning_rate</span> <span class="op">*</span> <span class="va">w1</span><span class="op">$</span><span class="va">grad</span><span class="op">)</span>
      <span class="va">w2</span><span class="op">$</span><span class="fu">sub_</span><span class="op">(</span><span class="va">learning_rate</span> <span class="op">*</span> <span class="va">w2</span><span class="op">$</span><span class="va">grad</span><span class="op">)</span>
      
      <span class="co"># Manually zero the gradients after updating weights</span>
      <span class="va">w1</span><span class="op">$</span><span class="va">grad</span><span class="op">$</span><span class="fu">zero_</span><span class="op">(</span><span class="op">)</span>
      <span class="va">w2</span><span class="op">$</span><span class="va">grad</span><span class="op">$</span><span class="fu">zero_</span><span class="op">(</span><span class="op">)</span>
   <span class="op">}</span><span class="op">)</span>
<span class="op">}</span>
<span class="co">#&gt; Step: 1 : 30390018 </span>
<span class="co">#&gt; Step: 100 : 576.8492 </span>
<span class="co">#&gt; Step: 200 : 3.614999 </span>
<span class="co">#&gt; Step: 300 : 0.04254073 </span>
<span class="co">#&gt; Step: 400 : 0.00102863 </span>
<span class="co">#&gt; Step: 500 : 0.0001272913</span></pre></div>
<p>In the <a href="new-autograd-functions.html">next example</a> we will learn how to create new autograd functions.</p>
  </div>

  <div class="col-md-3 hidden-xs hidden-sm" id="pkgdown-sidebar">

      </div>

</div>



      <footer><div class="copyright">
  <p>Developed by Daniel Falbel, Javier Luraschi.</p>
</div>

<div class="pkgdown">
  <p>Site built with <a href="https://pkgdown.r-lib.org/">pkgdown</a> 1.6.1.</p>
</div>

      </footer>
</div>

  


  </body>
</html>
